{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "import yaml\n",
        "from tensorflow.keras import layers, activations, models, preprocessing, utils\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "from matplotlib import pyplot\n",
        "\n",
        "from procesar_texto import separar_oracion_tokens\n",
        "\n",
        "print(\"tensor version {}\".format(tf.version.VERSION))\n",
        "\n",
        "dir_path = \"data\"\n",
        "files_list = os.listdir(dir_path + os.sep)\n",
        "\n",
        "questions = list()\n",
        "resp = list()\n",
        "answers = list()\n",
        "vocab = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "print(\"Archivos a entrenar: {}\".format(files_list))\n",
        "for filepath in files_list:\n",
        "    if filepath.endswith(\".yaml\"):\n",
        "        stream = open(dir_path + os.sep + filepath, \"rb\")\n",
        "        docs = yaml.safe_load(stream)\n",
        "        conversations = docs[\"conversaciones\"]\n",
        "        for con in conversations:\n",
        "            input_text = con[0]\n",
        "            if len(con) > 2:\n",
        "                pregunta = input_text\n",
        "                pregunta = pregunta.lower()\n",
        "                # pregunta = re.sub(\"[^a-zA-Z]\", \" \", pregunta)\n",
        "                questions.append(pregunta)\n",
        "                res = con[1:]\n",
        "\n",
        "                ans = \"\"\n",
        "                for rep in res:\n",
        "                    r = str(rep).lower()\n",
        "                    # r = re.sub(\"[^a-zA-Z]\", \" \", rep)\n",
        "                    ans += \" \" + r\n",
        "                resp.append(ans)\n",
        "            elif len(con) > 1:\n",
        "                questions.append(input_text)\n",
        "                target_text = con[1]\n",
        "                resp.append(target_text)\n",
        "\n",
        "                for char in separar_oracion_tokens(input_text):\n",
        "                    if char not in input_characters:\n",
        "                        input_characters.add(char)\n",
        "                for char in separar_oracion_tokens(target_text):\n",
        "                    if char not in target_characters:\n",
        "                        target_characters.add(char)\n",
        "\n",
        "answers_with_tags = list()\n",
        "for i in range(len(resp)):\n",
        "    if type(resp[i]) == str:\n",
        "        answers_with_tags.append(resp[i])\n",
        "    else:\n",
        "        questions.pop(i)\n",
        "\n",
        "for i in range(len(answers_with_tags)):\n",
        "    answers.append(\"<START> \" + answers_with_tags[i] + \" <END>\")\n",
        "\n",
        "palabras = set()\n",
        "palabras.add('start')\n",
        "palabras.add('end')\n",
        "palabras = palabras.union(target_characters)\n",
        "palabras = palabras.union(input_characters)\n",
        "palabras = sorted(list(palabras))\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(palabras)\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "print(\"vocab size : {}\".format(VOCAB_SIZE))\n",
        "\n",
        "for word in tokenizer.word_index:\n",
        "    if \"mediante\" in word:\n",
        "        print(\"aqui vocab\")\n",
        "    vocab.append(word)\n",
        "\n",
        "\n",
        "# Embedding?\n",
        "def tokenize(sentences):\n",
        "    tokens_list = []\n",
        "    vocabulary = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "        sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
        "        tokens = sentence.split()\n",
        "        vocabulary += tokens\n",
        "        tokens_list.append(tokens)\n",
        "\n",
        "    for i, tok in enumerate(tokens_list):\n",
        "        if tok == \"mediante\":\n",
        "            print(\"aqui\")\n",
        "    return tokens_list, vocabulary\n",
        "\n",
        "\n",
        "p = tokenize(questions + answers)\n",
        "model = Word2Vec(p[0])\n",
        "\n",
        "embedding_matrix = np.zeros((VOCAB_SIZE, 100))\n",
        "for i in range(len(tokenizer.word_index)):\n",
        "    try:\n",
        "        embedding_matrix[i] = model[vocab[i]]\n",
        "    except KeyError as identifier:\n",
        "        print(\"palabra no en vocab: {}\".format(identifier))\n",
        "\n",
        "# encoder_input_data\n",
        "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
        "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
        "padded_questions = preprocessing.sequence.pad_sequences(\n",
        "    tokenized_questions, maxlen=maxlen_questions, padding=\"post\"\n",
        ")\n",
        "encoder_input_data = np.array(padded_questions)\n",
        "print(encoder_input_data.shape, maxlen_questions)\n",
        "\n",
        "# decoder_input_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
        "padded_answers = preprocessing.sequence.pad_sequences(\n",
        "    tokenized_answers, maxlen=maxlen_answers, padding=\"post\"\n",
        ")\n",
        "decoder_input_data = np.array(padded_answers)\n",
        "print(decoder_input_data.shape, maxlen_answers)\n",
        "\n",
        "# decoder_output_data\n",
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "for i in range(len(tokenized_answers)):\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = preprocessing.sequence.pad_sequences(\n",
        "    tokenized_answers, maxlen=maxlen_answers, padding=\"post\"\n",
        ")\n",
        "onehot_answers = utils.to_categorical(padded_answers, VOCAB_SIZE)\n",
        "decoder_output_data = np.array(onehot_answers)\n",
        "print(decoder_output_data.shape)\n",
        "\n",
        "# Saving all the arrays to storage\n",
        "np.save(\"enc_in_data.npy\", encoder_input_data)\n",
        "np.save(\"dec_in_data.npy\", decoder_input_data)\n",
        "np.save(\"dec_tar_data.npy\", decoder_output_data)\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200, mask_zero=True)(\n",
        "    encoder_inputs\n",
        ")\n",
        "encoder_outputs, state_h, state_c = tf.keras.layers.LSTM(200, return_state=True)(\n",
        "    encoder_embedding\n",
        ")\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200, mask_zero=True)(\n",
        "    decoder_inputs\n",
        ")\n",
        "decoder_lstm = tf.keras.layers.LSTM(200, return_state=True, return_sequences=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = tf.keras.layers.Dense(\n",
        "    VOCAB_SIZE, activation=tf.keras.activations.softmax\n",
        ")\n",
        "output = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.RMSprop(),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_output_data,\n",
        "    validation_split=0.2,\n",
        "    verbose=1,\n",
        "    batch_size=32,\n",
        "    epochs=110,\n",
        ")\n",
        "\n",
        "# print(history.history['loss'])\n",
        "pyplot.plot(history.history['loss'])\n",
        "pyplot.plot(history.history['val_loss'])\n",
        "pyplot.title('model train vs validation loss')\n",
        "pyplot.ylabel('loss')\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.legend(['train', 'validation'], loc='upper right')\n",
        "pyplot.show()\n",
        "\n",
        "pyplot.plot(history.history['accuracy'])\n",
        "pyplot.plot(history.history['val_accuracy'])\n",
        "pyplot.title('model train vs validation accuracy')\n",
        "pyplot.ylabel('accuracy')\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.legend(['train', 'validation'], loc='upper right')\n",
        "pyplot.show()\n",
        "\n",
        "model.save(\"model.h5\")\n",
        "\n",
        "\n",
        "def make_inference_models():\n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=(200,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=(200,))\n",
        "\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding, initial_state=decoder_states_inputs\n",
        "    )\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "    )\n",
        "\n",
        "    return encoder_model, decoder_model\n",
        "\n",
        "\n",
        "def str_to_tokens(sentence: str):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append(tokenizer.word_index[word])\n",
        "    return preprocessing.sequence.pad_sequences(\n",
        "        [tokens_list], maxlen=maxlen_questions, padding=\"post\"\n",
        "    )\n",
        "\n",
        "\n",
        "enc_model, dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(10000):\n",
        "    states_values = enc_model.predict(str_to_tokens(input(\"Enter question : \")))\n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index[\"start\"]\n",
        "    stop_condition = False\n",
        "    decoded_translation = \"\"\n",
        "    while not stop_condition:\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq] + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if sampled_word_index == index:\n",
        "                decoded_translation += \" {}\".format(word)\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == \"end\":\n",
        "            stop_condition = True\n",
        "\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c]\n",
        "\n",
        "    print(decoded_translation)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}